{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the numpy and pandas packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prefix         0\n",
       "Assignment     2\n",
       "Tutorial      12\n",
       "Midterm       16\n",
       "TakeHome       9\n",
       "Final          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "marks = pd.read_csv('https://query.data.world/s/HqjNNadqEnwSq1qnoV_JqyRJkc7o6O')\n",
    "marks.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prefix         0\n",
       "Assignment     0\n",
       "Tutorial      10\n",
       "Midterm       14\n",
       "TakeHome       7\n",
       "Final          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your method needs to be fixed \n",
    "marks.drop(marks.index[(marks.isnull().sum(axis=1)== 5)]).isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prefix         0\n",
       "Assignment     0\n",
       "Tutorial      10\n",
       "Midterm       14\n",
       "TakeHome       7\n",
       "Final          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "marks = pd.read_csv('https://query.data.world/s/HqjNNadqEnwSq1qnoV_JqyRJkc7o6O')\n",
    "\n",
    "marks = marks[(marks.isnull().sum(axis=1) < 5) | (marks.isnull().sum(axis=1) >5) ]\n",
    "marks.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Reading and Inspection\n",
    "\n",
    "-  ### Subtask 1.1: Import and read\n",
    "\n",
    "Import and read the movie database. Store it in a variable called `movies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Movie+Assignment+Data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8a7aa6326bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Read the data from the csv using the pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Movie+Assignment+Data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#inspect data in the movies dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Movie+Assignment+Data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Read the data from the csv using the pandas\n",
    "movies = pd.read_csv(\"Movie+Assignment+Data.csv\")\n",
    "\n",
    "#inspect data in the movies dataframe\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 1.2: Inspect the dataframe\n",
    "\n",
    "Inspect the dataframe's columns, shapes, variable types etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for inspection here\n",
    "\n",
    "#print all the columns\n",
    "print(movies.columns)\n",
    "\n",
    "#print all the shape\n",
    "print(movies.shape)\n",
    "\n",
    "#print all the datatypes\n",
    "print(movies.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cleaning the Data\n",
    "\n",
    "-  ### Subtask 2.1: Inspect Null values\n",
    "\n",
    "Find out the number of Null values in all the columns and rows. Also, find the percentage of Null values in each column. Round off the percentages upto two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for column-wise null count here\n",
    "# axis 0 is column ,but by default the sum operates on column\n",
    "movies.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for row-wise null count here\n",
    "# axis 1 is for Rows \n",
    "movies.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for column-wise null percentages here\n",
    "# FYI Rounding is explicitly not mentioned ,hence not putting otherwise round(100*(movies.isnull().sum(axis=0)/len(movies.index)),2)\n",
    "100*(movies.isnull().sum(axis=0)/len(movies.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.2: Drop unecessary columns\n",
    "\n",
    "For this assignment, you will mostly be analyzing the movies with respect to the ratings, gross collection, popularity of movies, etc. So many of the columns in this dataframe are not required. So it is advised to drop the following columns.\n",
    "-  color\n",
    "-  director_facebook_likes\n",
    "-  actor_1_facebook_likes\n",
    "-  actor_2_facebook_likes\n",
    "-  actor_3_facebook_likes\n",
    "-  actor_2_name\n",
    "-  cast_total_facebook_likes\n",
    "-  actor_3_name\n",
    "-  duration\n",
    "-  facenumber_in_poster\n",
    "-  content_rating\n",
    "-  country\n",
    "-  movie_imdb_link\n",
    "-  aspect_ratio\n",
    "-  plot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for dropping the columns here. It is advised to keep inspecting the dataframe after each set of operations \n",
    "\n",
    "#create a list of the columns to be dropped\n",
    "columns_to_drop = [\"color\",\"director_facebook_likes\",\"actor_1_facebook_likes\",\"actor_2_facebook_likes\",\"actor_3_facebook_likes\",\"actor_2_name\",\"cast_total_facebook_likes\",\"actor_3_name\",\"duration\",\"facenumber_in_poster\",\"content_rating\",\"country\",\"movie_imdb_link\",\"aspect_ratio\",\"plot_keywords\"]\n",
    "\n",
    "#Based on the column list ,drop the column from the dataframe\n",
    "movies = movies.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#inspecting the dataframe after operation\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.3: Drop unecessary rows using columns with high Null percentages\n",
    "\n",
    "Now, on inspection you might notice that some columns have large percentage (greater than 5%) of Null values. Drop all the rows which have Null values for such columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for dropping the rows here\n",
    "#fetch all the column which has nulls percentage greater than 5% in list_of_columns\n",
    "list_of_columns = movies.isnull().sum(axis=0)/len(movies.index)*100>5\n",
    "list_of_columns = list_of_columns[list_of_columns[:,] == True]\n",
    "\n",
    "#loop through all the columns and check for null values in the rows of that column \n",
    "for column in list_of_columns.index.values:\n",
    "    movies = movies[~np.isnan(movies[column])]\n",
    "\n",
    "#inspect the dataframe after removing data\n",
    "  #Initially 5043 rows and after operation 3891 are left \n",
    "len(movies.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.4: Drop unecessary rows\n",
    "\n",
    "Some of the rows might have greater than five NaN values. Such rows aren't of much use for the analysis and hence, should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for dropping the rows here\n",
    "movies = movies[movies.isnull().sum(axis=1) <= 5]\n",
    "\n",
    "#inspect rows after operation of  deletion\n",
    "   #Note no rows get deleted \n",
    "len(movies.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.5: Fill NaN values\n",
    "\n",
    "You might notice that the `language` column has some NaN values. Here, on inspection, you will see that it is safe to replace all the missing values with `'English'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for filling the NaN values in the 'language' column here\n",
    "# impute NaN with English \n",
    "  #before imputing only 3 records exist with NaN ,hence replaced them with English\n",
    "movies[\"language\"].fillna(\"English\", inplace=True)\n",
    "\n",
    "#find the number of NaN records post imputing\n",
    "movies[movies[\"language\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.6: Check the number of retained rows\n",
    "\n",
    "You might notice that two of the columns viz. `num_critic_for_reviews` and `actor_1_name` have small percentages of NaN values left. You can let these columns as it is for now. Check the number and percentage of the rows retained after completing all the tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for checking number of retained rows here\n",
    "#Rounding is not mentioned in the task ,hence not applying the same\n",
    "print(len(movies.index))\n",
    "print((len(movies.index)/5043)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 1:** You might have noticed that we still have around `77%` of the rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data Analysis\n",
    "\n",
    "-  ### Subtask 3.1: Change the unit of columns\n",
    "\n",
    "Convert the unit of the `budget` and `gross` columns from `$` to `million $`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for unit conversion here\n",
    "movies['gross'] = movies['gross']/1000000\n",
    "movies['budget'] = movies['budget']/1000000\n",
    "\n",
    "#inspect column added after operations\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.2: Find the movies with highest profit\n",
    "\n",
    "    1. Create a new column called `profit` which contains the difference of the two columns: `gross` and `budget`.\n",
    "    2. Sort the dataframe using the `profit` column as reference.\n",
    "    3. Extract the top ten profiting movies in descending order and store them in a new dataframe - `top10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for creating the profit column here\n",
    "movies['profit'] = movies['gross'] - movies['budget']\n",
    "\n",
    "#inspect column added after operations\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for sorting the dataframe here\n",
    "movies.sort_values(by=['profit'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to get the top 10 profiting movies here\n",
    "top10 = movies.sort_values(by=['profit'], ascending=False).iloc[:10,]\n",
    "\n",
    "#inspect the dataframe\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.3: Drop duplicate values\n",
    "\n",
    "After you found out the top 10 profiting movies, you might have notice a duplicate value. So, it seems like the dataframe has duplicate values as well. Drop the duplicate values from the dataframe and repeat `Subtask 3.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for dropping duplicate values here\n",
    "top10.drop_duplicates(keep = False, inplace = True) \n",
    "# The total records after removing the duplicates is 8 ,code used is \n",
    "len(top10.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for repeating subtask 2 here\n",
    "top10 = movies.sort_values(by=['profit'], ascending=False).iloc[:10,]\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 2:** You might spot two movies directed by `James Cameron` in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.4: Find IMDb Top 250\n",
    "\n",
    "    1. Create a new dataframe `IMDb_Top_250` and store the top 250 movies with the highest IMDb Rating (corresponding to the column: `imdb_score`). Also make sure that for all of these movies, the `num_voted_users` is greater than 25,000.\n",
    "Also add a `Rank` column containing the values 1 to 250 indicating the ranks of the corresponding films.\n",
    "    2. Extract all the movies in the `IMDb_Top_250` dataframe which are not in the English language and store them in a new dataframe named `Top_Foreign_Lang_Film`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for extracting the top 250 movies as per the IMDb score here. Make sure that you store it in a new dataframe \n",
    "# and name that dataframe as 'IMDb_Top_250'\n",
    "\n",
    "#The new dataframe first is filtered on the num_voted_users and sorted by imdb_score and restricted till 250\n",
    "IMDb_Top_250 = movies[movies['num_voted_users']>25000].sort_values(by=['imdb_score'], ascending=False).iloc[:250,]\n",
    "\n",
    "#It is not clear ,on what basis we need to rank the dataset ,hence i assume imdb_score for that \n",
    "IMDb_Top_250['rank'] = IMDb_Top_250['imdb_score'].rank(method='dense',ascending=False).astype(int)\n",
    "\n",
    "#inpect the dataset post operations \n",
    "IMDb_Top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to extract top foreign language films from 'IMDb_Top_250' here\n",
    "Top_Foreign_Lang_Film = IMDb_Top_250[IMDb_Top_250['language']!=\"English\"]\n",
    "\n",
    "#Inspecting the dataset\n",
    "Top_Foreign_Lang_Film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 3:** Can you spot `Veer-Zaara` in the dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Subtask 3.5: Find the best directors\n",
    "\n",
    "    1. Group the dataframe using the `director_name` column.\n",
    "    2. Find out the top 10 directors for whom the mean of `imdb_score` is the highest and store them in a new dataframe `top10director`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for extracting the top 10 directors here\n",
    "\n",
    "# Grouping: First, we will group the dataframe by director_name\n",
    "director_name_group = movies.groupby('director_name')\n",
    "\n",
    "#fetch the top ten directory based on the imdb_score mean \n",
    "top10director = director_name_group['imdb_score'].mean().sort_values(ascending = False).iloc[:10,]\n",
    "\n",
    "#Inspect the dataframe\n",
    "top10director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 4:** No surprises that `Damien Chazelle` (director of Whiplash and La La Land) is in this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.6: Find popular genres\n",
    "\n",
    "You might have noticed the `genres` column in the dataframe with all the genres of the movies seperated by a pipe (`|`). Out of all the movie genres, the first two are most significant for any film.\n",
    "\n",
    "1. Extract the first two genres from the `genres` column and store them in two new columns: `genre_1` and `genre_2`. Some of the movies might have only one genre. In such cases, extract the single genre into both the columns, i.e. for such movies the `genre_2` will be the same as `genre_1`.\n",
    "2. Group the dataframe using `genre_1` as the primary column and `genre_2` as the secondary column.\n",
    "3. Find out the 5 most popular combo of genres by finding the mean of the gross values using the `gross` column and store them in a new dataframe named `PopGenre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for extracting the first two genres of each movie here\n",
    "movies['genre_1'] = movies['genres'].str.split('|').str[0]\n",
    "movies['genre_2'] = movies['genres'].str.split('|').str[1].fillna(movies['genre_1'], inplace=False)\n",
    "\n",
    "#Inspecting the dataframe\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code for grouping the dataframe here\n",
    "movies_by_genre = movies.groupby(['genre_1', 'genre_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for getting the 5 most popular combo of genres here\n",
    "PopGenre = movies_by_genre['gross'].mean().sort_values(ascending = False).iloc[:5,]\n",
    "\n",
    "#Inspecting the dataset\n",
    "PopGenre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 5:** Well, as it turns out. `Family + Sci-Fi` is the most popular combo of genres out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.7: Find the critic-favorite and audience-favorite actors\n",
    "\n",
    "    1. Create three new dataframes namely, `Meryl_Streep`, `Leo_Caprio`, and `Brad_Pitt` which contain the movies in which the actors: 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' are the lead actors. Use only the `actor_1_name` column for extraction. Also, make sure that you use the names 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' for the said extraction.\n",
    "    2. Append the rows of all these dataframes and store them in a new dataframe named `Combined`.\n",
    "    3. Group the combined dataframe using the `actor_1_name` column.\n",
    "    4. Find the mean of the `num_critic_for_reviews` and `num_user_for_review` and identify the actors which have the highest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for creating three new dataframes here\n",
    "# Include all movies in which Meryl_Streep is the lead\n",
    "Meryl_Streep = movies[movies['actor_1_name']=='Meryl Streep']\n",
    "\n",
    "#Inspecting the dataframe\n",
    "Meryl_Streep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all movies in which Leo_Caprio is the lead\n",
    "Leo_Caprio = movies[movies['actor_1_name']=='Leonardo DiCaprio']\n",
    "\n",
    "#Inspecting the dataframe\n",
    "Leo_Caprio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all movies in which Brad_Pitt is the lead\n",
    "Brad_Pitt = movies[movies['actor_1_name']=='Brad Pitt']\n",
    "\n",
    "#Inspecting the dataframe\n",
    "Brad_Pitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for combining the three dataframes here\n",
    "Combined = pd.concat([Meryl_Streep, Leo_Caprio, Brad_Pitt], axis = 0)\n",
    "\n",
    "#Inspecting the dataframe\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code for grouping the combined dataframe here\n",
    "Combined_by_actor_name = Combined.groupby(['actor_1_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code for finding the mean of critic reviews and audience reviews here\n",
    "Combined_by_actor_name[['num_critic_for_reviews', 'num_user_for_reviews']].mean().sort_values(by=['num_critic_for_reviews', 'num_user_for_reviews'],ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint 6:** `Leonardo` has aced both the lists!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
